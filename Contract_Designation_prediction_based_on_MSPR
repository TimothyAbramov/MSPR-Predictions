{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxpAq1UchwIJ",
        "outputId": "cca3d951-b570-4774-9355-e2e52c3f655a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# prompt: mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Putting data together**"
      ],
      "metadata": {
        "id": "eJp3hEwvuBCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import a .csv dataset\n",
        "\n",
        "import pandas as pd\n",
        "mvp_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DL Fall 2024/MSPR predictions/datasets/mvp_train(missing 2018-2019).csv') #Change path if needed\n",
        "print(mvp_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-CfDbUMh7gU",
        "outputId": "6a63811c-326f-40c8-c946-e082e993ba34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ATTENDANCE  LOW PARTICIPATION  LATE/MISSING ASSIGNMENTS  \\\n",
            "0           0                  0                         0   \n",
            "1           0                  0                         0   \n",
            "2           0                  0                         0   \n",
            "3           0                  0                         0   \n",
            "4           0                  0                         0   \n",
            "\n",
            "   OTHER ASSIGNMENTS CONCERNS  LOW TEST SCORES  DANGER of UNSATING  \\\n",
            "0                           0                0                   0   \n",
            "1                           0                0                   0   \n",
            "2                           0                0                   0   \n",
            "3                           0                0                   0   \n",
            "4                           0                0                   0   \n",
            "\n",
            "   DESIGNATION_BINARY  \n",
            "0                   1  \n",
            "1                   0  \n",
            "2                   1  \n",
            "3                   1  \n",
            "4                   1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mvp_df.info()"
      ],
      "metadata": {
        "id": "MDyQvr1L7mQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a357ff2b-f41a-4968-8d49-24e0971d9ff4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3690 entries, 0 to 3689\n",
            "Data columns (total 7 columns):\n",
            " #   Column                      Non-Null Count  Dtype\n",
            "---  ------                      --------------  -----\n",
            " 0   ATTENDANCE                  3690 non-null   int64\n",
            " 1   LOW PARTICIPATION           3690 non-null   int64\n",
            " 2   LATE/MISSING ASSIGNMENTS    3690 non-null   int64\n",
            " 3   OTHER ASSIGNMENTS CONCERNS  3690 non-null   int64\n",
            " 4   LOW TEST SCORES             3690 non-null   int64\n",
            " 5   DANGER of UNSATING          3690 non-null   int64\n",
            " 6   DESIGNATION_BINARY          3690 non-null   int64\n",
            "dtypes: int64(7)\n",
            "memory usage: 201.9 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: show distributions for fields in the mvp_df, show distributions in terms of count and percentage\n",
        "\n",
        "# Calculate counts and percentages for each column\n",
        "for col in mvp_df.columns:\n",
        "  print(f\"Distribution for column: {col}\")\n",
        "  counts = mvp_df[col].value_counts()\n",
        "  percentages = (counts / len(mvp_df)) * 100\n",
        "  distribution_df = pd.DataFrame({'Count': counts, 'Percentage': percentages})\n",
        "  print(distribution_df)\n",
        "  print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksymMAIiBEGJ",
        "outputId": "e8846465-8d70-426a-ba0c-0e6959f470a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution for column: ATTENDANCE\n",
            "            Count  Percentage\n",
            "ATTENDANCE                   \n",
            "0            3186   86.341463\n",
            "1             369   10.000000\n",
            "2              88    2.384824\n",
            "3              38    1.029810\n",
            "4               8    0.216802\n",
            "5               1    0.027100\n",
            "--------------------\n",
            "Distribution for column: LOW PARTICIPATION\n",
            "                   Count  Percentage\n",
            "LOW PARTICIPATION                   \n",
            "0                   3157   85.555556\n",
            "1                    445   12.059621\n",
            "2                     73    1.978320\n",
            "3                     15    0.406504\n",
            "--------------------\n",
            "Distribution for column: LATE/MISSING ASSIGNMENTS\n",
            "                          Count  Percentage\n",
            "LATE/MISSING ASSIGNMENTS                   \n",
            "0                          2677   72.547425\n",
            "1                           671   18.184282\n",
            "2                           234    6.341463\n",
            "3                            95    2.574526\n",
            "4                            12    0.325203\n",
            "5                             1    0.027100\n",
            "--------------------\n",
            "Distribution for column: OTHER ASSIGNMENTS CONCERNS\n",
            "                            Count  Percentage\n",
            "OTHER ASSIGNMENTS CONCERNS                   \n",
            "0                            3314   89.810298\n",
            "1                             338    9.159892\n",
            "2                              33    0.894309\n",
            "3                               5    0.135501\n",
            "--------------------\n",
            "Distribution for column: LOW TEST SCORES\n",
            "                 Count  Percentage\n",
            "LOW TEST SCORES                   \n",
            "0                 3162   85.691057\n",
            "1                  448   12.140921\n",
            "2                   74    2.005420\n",
            "3                    6    0.162602\n",
            "--------------------\n",
            "Distribution for column: DANGER of UNSATING\n",
            "                    Count  Percentage\n",
            "DANGER of UNSATING                   \n",
            "0                    3145   85.230352\n",
            "1                     407   11.029810\n",
            "2                      98    2.655827\n",
            "3                      33    0.894309\n",
            "4                       6    0.162602\n",
            "5                       1    0.027100\n",
            "--------------------\n",
            "Distribution for column: DESIGNATION_BINARY\n",
            "                    Count  Percentage\n",
            "DESIGNATION_BINARY                   \n",
            "1                    3378   91.544715\n",
            "0                     312    8.455285\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stratified sampling for train, val, and test data**"
      ],
      "metadata": {
        "id": "JdDVChsuudys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Split mvp_df into train, validation, and test. Use stratified splitting to maintain class balance across sets.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'TargetVariable' is the column you want to stratify by\n",
        "# Replace 'TargetVariable' with the actual name of your target variable column\n",
        "\n",
        "train_df, temp_df = train_test_split(mvp_df, test_size=0.3, stratify=mvp_df['DESIGNATION_BINARY'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['DESIGNATION_BINARY'], random_state=42)\n",
        "\n",
        "print(\"Train set shape:\", train_df.shape)\n",
        "print(\"Validation set shape:\", val_df.shape)\n",
        "print(\"Test set shape:\", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yc_A-_6BszT",
        "outputId": "2ea387ec-8a53-4a7b-f22d-6aa3d84567e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (2583, 7)\n",
            "Validation set shape: (553, 7)\n",
            "Test set shape: (554, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Show distributions of DESIGNATION_BINARY in train_df, val_df, and test_df\n",
        "\n",
        "def show_distribution(df, name):\n",
        "    print(f\"Distribution for {name}:\")\n",
        "    counts = df['DESIGNATION_BINARY'].value_counts()\n",
        "    percentages = (counts / len(df)) * 100\n",
        "    distribution_df = pd.DataFrame({'Count': counts, 'Percentage': percentages})\n",
        "    print(distribution_df)\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "show_distribution(train_df, \"train_df\")\n",
        "show_distribution(val_df, \"val_df\")\n",
        "show_distribution(test_df, \"test_df\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ZxXfnRu4gY",
        "outputId": "caefced6-22ba-4aa4-a066-93cb048ee766"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution for train_df:\n",
            "                    Count  Percentage\n",
            "DESIGNATION_BINARY                   \n",
            "1                    2365   91.560201\n",
            "0                     218    8.439799\n",
            "--------------------\n",
            "Distribution for val_df:\n",
            "                    Count  Percentage\n",
            "DESIGNATION_BINARY                   \n",
            "1                     506   91.500904\n",
            "0                      47    8.499096\n",
            "--------------------\n",
            "Distribution for test_df:\n",
            "                    Count  Percentage\n",
            "DESIGNATION_BINARY                   \n",
            "1                     507   91.516245\n",
            "0                      47    8.483755\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Split train_df into X_train and y_train. Split val_df into X_val and y_val. Split test_df into X_test and y_test.\n",
        "\n",
        "X_train = train_df.drop('DESIGNATION_BINARY', axis=1)\n",
        "y_train = train_df['DESIGNATION_BINARY']\n",
        "\n",
        "X_val = val_df.drop('DESIGNATION_BINARY', axis=1)\n",
        "y_val = val_df['DESIGNATION_BINARY']\n",
        "\n",
        "X_test = test_df.drop('DESIGNATION_BINARY', axis=1)\n",
        "y_test = test_df['DESIGNATION_BINARY']"
      ],
      "metadata": {
        "id": "lrSnJtAqCzxI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standartization**"
      ],
      "metadata": {
        "id": "jmuhUT5HuNNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Standardize X_train. Then standardize X_val and X_test. Use from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on X_train and transform\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform X_val and X_test using the same scaler\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "veYpwX1HuA2k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Print mean and std of X_train_scaled, X_val_scaled, and X_test_scaled\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(\"Mean of X_train_scaled:\", np.mean(X_train_scaled))\n",
        "print(\"Standard deviation of X_train_scaled:\", np.std(X_train_scaled))\n",
        "\n",
        "print(\"Mean of X_val_scaled:\", np.mean(X_val_scaled))\n",
        "print(\"Standard deviation of X_val_scaled:\", np.std(X_val_scaled))\n",
        "\n",
        "print(\"Mean of X_test_scaled:\", np.mean(X_test_scaled))\n",
        "print(\"Standard deviation of X_test_scaled:\", np.std(X_test_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P90Y_LmiuOlt",
        "outputId": "41a7b251-bf03-4ff2-a9ed-636b74a99615"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of X_train_scaled: -1.0315661088270909e-17\n",
            "Standard deviation of X_train_scaled: 1.0\n",
            "Mean of X_val_scaled: -0.04772855839980563\n",
            "Standard deviation of X_val_scaled: 0.9650442370384306\n",
            "Mean of X_test_scaled: -0.056699466174357845\n",
            "Standard deviation of X_test_scaled: 0.9104110779138294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SMOTE (balance classes for label)**"
      ],
      "metadata": {
        "id": "V_Ir38PPuTvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use SMOTE on X_train and y_train, to fix unbalance in DESIGNATION_BINARY class distribution\n",
        "\n",
        "!pip install imbalanced-learn\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training data\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print()\n",
        "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "print()\n",
        "print(\"Shape of X_train_resampled:\", X_train_resampled.shape)\n",
        "print(\"Shape of y_train_resampled:\", y_train_resampled.shape)\n",
        "\n",
        "print()\n",
        "print(\"Distribution of y_train_resampled:\")\n",
        "print(y_train_resampled.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDUnZd_9vFP6",
        "outputId": "bbf5fca8-f03c-490a-ff00-ed1d2552222a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "\n",
            "Shape of X_train_scaled: (2583, 6)\n",
            "Shape of y_train: (2583,)\n",
            "\n",
            "Shape of X_train_resampled: (4730, 6)\n",
            "Shape of y_train_resampled: (4730,)\n",
            "\n",
            "Distribution of y_train_resampled:\n",
            "DESIGNATION_BINARY\n",
            "1    2365\n",
            "0    2365\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline model: Logistic**"
      ],
      "metadata": {
        "id": "xXT5DAo3ujwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Fit a Logistic regression on train data sets, validate on the validation data sets. Record performance metrics for further comparison.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "logreg_model = LogisticRegression(random_state=42)\n",
        "logreg_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = logreg_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred)\n",
        "recall = recall_score(y_val, y_val_pred)\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "roc_auc = roc_auc_score(y_val, y_val_pred)\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Performance Metrics (Validation Set):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "performance_metrics = {\n",
        "    'model': 'Logistic Regression',\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1_score': f1,\n",
        "    'roc_auc': roc_auc\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlJThwn1yGyu",
        "outputId": "c0afe360-ecdf-4372-a225-ca4e83b0d479"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Performance Metrics (Validation Set):\n",
            "Accuracy: 0.8427\n",
            "Precision: 0.9564\n",
            "Recall: 0.8676\n",
            "F1-score: 0.9098\n",
            "ROC AUC: 0.7210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Model: Tuning**"
      ],
      "metadata": {
        "id": "kvvH7dy2uqnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use grid search and cross validation for hyperparameter tuning for the Logistic regression. Record performance metrics.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for Logistic Regression\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'solver': ['liblinear', 'saga'] # Solvers that support both penalties\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000), param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "\n",
        "# Get the best hyperparameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "y_val_pred = best_model.predict(X_val_scaled)\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred)\n",
        "recall = recall_score(y_val, y_val_pred)\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "roc_auc = roc_auc_score(y_val, y_val_pred)\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Performance Metrics (Validation Set - After Tuning):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "performance_metrics = {\n",
        "    'model': 'Tuned Logistic Regression',\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1_score': f1,\n",
        "    'roc_auc': roc_auc\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArD0l4BOy934",
        "outputId": "795e0e8a-97a1-4806-aaca-d2da4f766159"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Logistic Regression Performance Metrics (Validation Set - After Tuning):\n",
            "Accuracy: 0.8427\n",
            "Precision: 0.9564\n",
            "Recall: 0.8676\n",
            "F1-score: 0.9098\n",
            "ROC AUC: 0.7210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network for binary classification**"
      ],
      "metadata": {
        "id": "BpMraGk65F8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Fit a Neural Network that would be well suited for binary classification on a relatively small dataset. Fit on train data, evaluate on the validation data. Record performance metrics.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
        "    keras.layers.Dropout(0.5),  # Add dropout for regularization\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dropout(0.3),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_resampled, y_train_resampled, epochs=50, batch_size=32, validation_data=(X_val_scaled, y_val))\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred_prob = model.predict(X_val_scaled)\n",
        "y_val_pred = (y_val_pred_prob > 0.5).astype(int) # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluate the model (validation set)\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred)\n",
        "recall = recall_score(y_val, y_val_pred)\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
        "\n",
        "\n",
        "print(\"Neural Network Performance Metrics (Validation Set):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "performance_metrics = {\n",
        "    'model': 'Neural Network',\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall,\n",
        "    'f1_score': f1,\n",
        "    'roc_auc': roc_auc\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_I2rtfA5OTJ",
        "outputId": "b899c2da-4c4a-4c46-cdbf-072462724db2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6743 - loss: 0.5996 - val_accuracy: 0.8119 - val_loss: 0.4791\n",
            "Epoch 2/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7733 - loss: 0.5145 - val_accuracy: 0.8354 - val_loss: 0.4434\n",
            "Epoch 3/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7816 - loss: 0.5062 - val_accuracy: 0.8336 - val_loss: 0.4562\n",
            "Epoch 4/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8018 - loss: 0.4795 - val_accuracy: 0.8336 - val_loss: 0.4445\n",
            "Epoch 5/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7841 - loss: 0.5024 - val_accuracy: 0.8354 - val_loss: 0.4419\n",
            "Epoch 6/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8056 - loss: 0.4771 - val_accuracy: 0.8318 - val_loss: 0.4594\n",
            "Epoch 7/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7964 - loss: 0.4842 - val_accuracy: 0.8336 - val_loss: 0.4309\n",
            "Epoch 8/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8095 - loss: 0.4744 - val_accuracy: 0.8354 - val_loss: 0.4292\n",
            "Epoch 9/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8052 - loss: 0.4680 - val_accuracy: 0.8246 - val_loss: 0.4459\n",
            "Epoch 10/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8027 - loss: 0.4752 - val_accuracy: 0.8373 - val_loss: 0.4179\n",
            "Epoch 11/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7988 - loss: 0.4869 - val_accuracy: 0.8246 - val_loss: 0.4237\n",
            "Epoch 12/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7996 - loss: 0.4735 - val_accuracy: 0.8246 - val_loss: 0.4189\n",
            "Epoch 13/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8044 - loss: 0.4809 - val_accuracy: 0.8264 - val_loss: 0.4203\n",
            "Epoch 14/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8052 - loss: 0.4755 - val_accuracy: 0.8264 - val_loss: 0.4248\n",
            "Epoch 15/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8091 - loss: 0.4682 - val_accuracy: 0.8282 - val_loss: 0.4397\n",
            "Epoch 16/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8150 - loss: 0.4540 - val_accuracy: 0.8354 - val_loss: 0.4072\n",
            "Epoch 17/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8049 - loss: 0.4703 - val_accuracy: 0.8336 - val_loss: 0.4009\n",
            "Epoch 18/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8029 - loss: 0.4723 - val_accuracy: 0.8336 - val_loss: 0.4257\n",
            "Epoch 19/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8039 - loss: 0.4649 - val_accuracy: 0.8336 - val_loss: 0.4289\n",
            "Epoch 20/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7956 - loss: 0.4802 - val_accuracy: 0.8318 - val_loss: 0.4308\n",
            "Epoch 21/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7998 - loss: 0.4812 - val_accuracy: 0.8336 - val_loss: 0.4261\n",
            "Epoch 22/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8033 - loss: 0.4734 - val_accuracy: 0.8427 - val_loss: 0.4103\n",
            "Epoch 23/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8103 - loss: 0.4680 - val_accuracy: 0.8354 - val_loss: 0.4136\n",
            "Epoch 24/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8021 - loss: 0.4648 - val_accuracy: 0.8336 - val_loss: 0.4254\n",
            "Epoch 25/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8204 - loss: 0.4464 - val_accuracy: 0.8354 - val_loss: 0.4250\n",
            "Epoch 26/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.4540 - val_accuracy: 0.8373 - val_loss: 0.4351\n",
            "Epoch 27/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8044 - loss: 0.4674 - val_accuracy: 0.8391 - val_loss: 0.4056\n",
            "Epoch 28/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7987 - loss: 0.4639 - val_accuracy: 0.8427 - val_loss: 0.4008\n",
            "Epoch 29/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8077 - loss: 0.4617 - val_accuracy: 0.8373 - val_loss: 0.4375\n",
            "Epoch 30/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8094 - loss: 0.4574 - val_accuracy: 0.8427 - val_loss: 0.4129\n",
            "Epoch 31/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8124 - loss: 0.4553 - val_accuracy: 0.8391 - val_loss: 0.4209\n",
            "Epoch 32/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8079 - loss: 0.4626 - val_accuracy: 0.8354 - val_loss: 0.4387\n",
            "Epoch 33/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8096 - loss: 0.4543 - val_accuracy: 0.8427 - val_loss: 0.4095\n",
            "Epoch 34/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8075 - loss: 0.4580 - val_accuracy: 0.8409 - val_loss: 0.4075\n",
            "Epoch 35/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8170 - loss: 0.4508 - val_accuracy: 0.8445 - val_loss: 0.4074\n",
            "Epoch 36/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8084 - loss: 0.4622 - val_accuracy: 0.8373 - val_loss: 0.4305\n",
            "Epoch 37/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8113 - loss: 0.4576 - val_accuracy: 0.8445 - val_loss: 0.4162\n",
            "Epoch 38/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8137 - loss: 0.4531 - val_accuracy: 0.8427 - val_loss: 0.4176\n",
            "Epoch 39/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8215 - loss: 0.4428 - val_accuracy: 0.8427 - val_loss: 0.4313\n",
            "Epoch 40/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8122 - loss: 0.4540 - val_accuracy: 0.8445 - val_loss: 0.4123\n",
            "Epoch 41/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8173 - loss: 0.4458 - val_accuracy: 0.8517 - val_loss: 0.4058\n",
            "Epoch 42/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8053 - loss: 0.4615 - val_accuracy: 0.8517 - val_loss: 0.4028\n",
            "Epoch 43/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8126 - loss: 0.4541 - val_accuracy: 0.8445 - val_loss: 0.4218\n",
            "Epoch 44/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8178 - loss: 0.4488 - val_accuracy: 0.8427 - val_loss: 0.4212\n",
            "Epoch 45/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8100 - loss: 0.4593 - val_accuracy: 0.8445 - val_loss: 0.4175\n",
            "Epoch 46/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8187 - loss: 0.4463 - val_accuracy: 0.8463 - val_loss: 0.4155\n",
            "Epoch 47/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8121 - loss: 0.4578 - val_accuracy: 0.8463 - val_loss: 0.4283\n",
            "Epoch 48/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8170 - loss: 0.4391 - val_accuracy: 0.8463 - val_loss: 0.4120\n",
            "Epoch 49/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8152 - loss: 0.4492 - val_accuracy: 0.8517 - val_loss: 0.4115\n",
            "Epoch 50/50\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8017 - loss: 0.4622 - val_accuracy: 0.8517 - val_loss: 0.3934\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Neural Network Performance Metrics (Validation Set):\n",
            "Accuracy: 0.8517\n",
            "Precision: 0.9629\n",
            "Recall: 0.8715\n",
            "F1-score: 0.9149\n",
            "ROC AUC: 0.7599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from tensorflow import keras\n",
        "\n",
        "# Ensure data is prepared correctly\n",
        "# X_train_resampled, y_train_resampled = ... (prepare your training data)\n",
        "# X_val_scaled, y_val = ... (prepare your validation data)\n",
        "\n",
        "# Define the parameter grid for the neural network\n",
        "param_grid = {\n",
        "    'units1': [32, 64, 128],\n",
        "    'units2': [16, 32, 64],\n",
        "    'dropout1': [0.3, 0.5, 0.7],\n",
        "    'dropout2': [0.2, 0.4, 0.6],\n",
        "    'optimizer': ['adam', 'rmsprop'],\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'epochs': [50]  # Ensure epochs are part of GridSearchCV\n",
        "}\n",
        "\n",
        "# Function to create and compile the model\n",
        "def create_model(units1, units2, dropout1, dropout2, optimizer):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(units1, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
        "        keras.layers.Dropout(dropout1),\n",
        "        keras.layers.Dense(units2, activation='relu'),\n",
        "        keras.layers.Dropout(dropout2),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Use KerasClassifier for Scikit-learn compatibility\n",
        "model = KerasClassifier(model=create_model, verbose=0)\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1)\n",
        "grid_result = grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "best_model = grid_result.best_estimator_\n",
        "y_val_pred_prob = best_model.predict_proba(X_val_scaled)\n",
        "y_val_pred = (y_val_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the model\n",
        "# Ensure the labels are binary\n",
        "y_val = LabelEncoder().fit_transform(y_val)\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "precision = precision_score(y_val, y_val_pred)\n",
        "recall = recall_score(y_val, y_val_pred)\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Tuned Neural Network Performance Metrics (Validation Set):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "_DObGpU5-J3a",
        "outputId": "90d1545b-9db3-42d4-f69d-82c43a769cb1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid parameter units1 for estimator KerasClassifier.\nThis issue can likely be resolved by setting this parameter in the KerasClassifier constructor:\n`KerasClassifier(units1=32)`\nCheck the list of available parameters with `estimator.get_params().keys()`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\", line 136, in __call__\n    return self.function(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 876, in _fit_and_score\n    estimator = estimator.set_params(**clone(parameters, safe=False))\n  File \"/usr/local/lib/python3.10/dist-packages/scikeras/wrappers.py\", line 1175, in set_params\n    raise ValueError(\nValueError: Invalid parameter units1 for estimator KerasClassifier.\nThis issue can likely be resolved by setting this parameter in the KerasClassifier constructor:\n`KerasClassifier(units1=32)`\nCheck the list of available parameters with `estimator.get_params().keys()`\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-683b64c5f9cf>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Use GridSearchCV to find the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Print the best parameters and score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    963\u001b[0m                     )\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    967\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid parameter units1 for estimator KerasClassifier.\nThis issue can likely be resolved by setting this parameter in the KerasClassifier constructor:\n`KerasClassifier(units1=32)`\nCheck the list of available parameters with `estimator.get_params().keys()`"
          ]
        }
      ]
    }
  ]
}